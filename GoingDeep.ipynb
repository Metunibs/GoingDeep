{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brewing Logistic Regression then Going Deeper\n",
    "\n",
    "While Caffe is made for deep networks it can likewise represent \"shallow\" models like logistic regression for classification. We'll do simple logistic regression on synthetic data that we'll generate and save to HDF5 to feed vectors to Caffe. Once that model is done, we'll add layers to improve accuracy. That's what Caffe is about: define a model, experiment, and then deploy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup\n",
    "\n",
    "* Python usual setup with `numpy`, and `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# display the plot inline in this notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load `caffe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The caffe module needs to be on the Python path;\n",
    "#  we'll add it here explicitly.\n",
    "import os\n",
    "os.chdir('..')\n",
    "import sys\n",
    "#caffe_root = '/home/NAME/caffe/'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
    "#sys.path.insert(0, caffe_root + 'python')\n",
    "\n",
    "import caffe # If you get \"No module named _caffe\", either you have not built pycaffe or you have the wrong path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load additional useful lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Visualize net\n",
    "from google.protobuf import text_format\n",
    "from caffe.draw import get_pydot_graph\n",
    "from caffe.proto import caffe_pb2\n",
    "from IPython.display import display, Image \n",
    "\n",
    "\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the dataset\n",
    "\n",
    "We load the dataset used in the previous lab *Data scientist for a day* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " train_data = pd.read_csv('./GoingDeep/train.csv', header=None)\n",
    " train_labels = pd.read_csv('./GoingDeep/trainLabels.csv', header=None)\n",
    " test_data = pd.read_csv('./GoingDeep/test.csv', header=None)\n",
    " X = np.asarray(train_data)\n",
    " y = np.asarray(train_labels).ravel()\n",
    " X_test= np.asarray(test_data)\n",
    "\n",
    "# Split into train and test\n",
    "X, Xt, y, yt = sklearn.cross_validation.train_test_split(X, y,test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data enters Caffe through data layers: they lie at the bottom of nets. Data can come from efficient databases (LevelDB or LMDB), directly from memory, or, when efficiency is not critical, from files on disk in HDF5 or common image formats.\n",
    "HDF5 is a Hierarchical Data Format consisting of a data format specification and a supporting library implementation. \n",
    "\n",
    "Next we save the dataset to HDF5 for loading in Caffe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write out the data to HDF5 files in a temp directory.\n",
    "# This file is assumed to be caffe_root/examples/hdf5_classification.ipynb\n",
    "dirname = os.path.abspath('./GoingDeep/hdf5_classification/data')\n",
    "if not os.path.exists(dirname):\n",
    "    os.makedirs(dirname)\n",
    "\n",
    "train_filename = os.path.join(dirname, 'train.h5')\n",
    "test_filename = os.path.join(dirname, 'test.h5')\n",
    "submission_filename = os.path.join(dirname, 'submission.h5')\n",
    "\n",
    "\n",
    "# HDF5DataLayer source should be a file containing a list of HDF5 filenames.\n",
    "# To show this off, we'll list the same data file twice.\n",
    "with h5py.File(train_filename, 'w') as f:\n",
    "    f['data'] = X\n",
    "    f['label'] = y.astype(np.float32)\n",
    "with open(os.path.join(dirname, 'train.txt'), 'w') as f:\n",
    "    f.write(train_filename + '\\n')\n",
    "    f.write(train_filename + '\\n')\n",
    "    \n",
    "# HDF5 is pretty efficient, but can be further compressed.\n",
    "comp_kwargs = {'compression': 'gzip', 'compression_opts': 1}\n",
    "with h5py.File(test_filename, 'w') as f:\n",
    "    f.create_dataset('data', data=Xt, **comp_kwargs)\n",
    "    f.create_dataset('label', data=yt.astype(np.float32), **comp_kwargs)\n",
    "with open(os.path.join(dirname, 'test.txt'), 'w') as f:\n",
    "    f.write(test_filename + '\\n')\n",
    "    \n",
    "with h5py.File(submission_filename, 'w') as f:\n",
    "    f.create_dataset('data', data=X_test, **comp_kwargs)\n",
    "with open(os.path.join(dirname, 'submission.txt'), 'w') as f:\n",
    "    f.write(submission_filename + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Simple logistic regressor\n",
    "\n",
    "Let's define logistic regression in Caffe through Python net specification. \n",
    "\n",
    "![Test](logistic.png \"Test\")\n",
    "\n",
    "This is a quick and natural way to define nets that sidesteps manually editing the protobuf model. A network is a set of layers and their connections. Caffe creates and checks the net from the definition.\n",
    "Data and derivatives flow through the net as blobs that is an array interface\n",
    "\n",
    "Blobs are N-D arrays for storing and communicating information.\n",
    "* hold data, derivatives, and parameters\n",
    "* lazily allocate memory\n",
    "* shuttle between CPU and GPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "\n",
    "def logreg(hdf5, batch_size):\n",
    "    # logistic regression: data, matrix multiplication, and 2-class softmax loss\n",
    "    n = caffe.NetSpec()\n",
    "    n.data, n.label = L.HDF5Data(batch_size=batch_size, source=hdf5, ntop=2)\n",
    "    n.ip1 = L.InnerProduct(n.data, num_output=2, weight_filler=dict(type='xavier'))\n",
    "    n.accuracy = L.Accuracy(n.ip1, n.label)\n",
    "    n.loss = L.SoftmaxWithLoss(n.ip1, n.label)\n",
    "    return n.to_proto()\n",
    "\n",
    "train_net_path = 'GoingDeep/hdf5_classification/logreg_auto_train.prototxt'\n",
    "with open(train_net_path, 'w') as f:\n",
    "    f.write(str(logreg('GoingDeep/hdf5_classification/data/train.txt', 10)))\n",
    "\n",
    "test_net_path = 'GoingDeep/hdf5_classification/logreg_auto_test.prototxt'\n",
    "with open(test_net_path, 'w') as f:\n",
    "    f.write(str(logreg('GoingDeep/hdf5_classification/data/test.txt', 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's visualize the network graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pydot_graph_(caffe_net):\n",
    "    pydot_graph = pydot.Dot(caffe_net.name, graph_type='digraph')\n",
    "    pydot_nodes = {}\n",
    "    pydot_edges = []\n",
    "    for layer in caffe_net.layers:\n",
    "        name = layer.layer.name\n",
    "        layertype = layer.layer.type\n",
    "        if (len(layer.bottom) == 1 and len(layer.top) == 1 and\n",
    "            layer.bottom[0] == layer.top[0]):\n",
    "          # We have an in-place neuron layer.\n",
    "          pydot_nodes[name + '_' + layertype] = pydot.Node(\n",
    "              '%s (%s)' % (name, layertype), **NEURON_LAYER_STYLE)\n",
    "        else:\n",
    "          pydot_nodes[name + '_' + layertype] = pydot.Node(\n",
    "              '%s (%s)' % (name, layertype), **LAYER_STYLE)\n",
    "        for bottom_blob in layer.bottom:\n",
    "          pydot_nodes[bottom_blob + '_blob'] = pydot.Node(\n",
    "            '%s' % (bottom_blob), **BLOB_STYLE)\n",
    "          pydot_edges.append((bottom_blob + '_blob', name + '_' + layertype))\n",
    "        for top_blob in layer.top:\n",
    "          pydot_nodes[top_blob + '_blob'] = pydot.Node(\n",
    "            '%s' % (top_blob))\n",
    "          pydot_edges.append((name + '_' + layertype, top_blob + '_blob'))\n",
    "    # Now, add the nodes and edges to the graph.\n",
    "    for node in pydot_nodes.values():\n",
    "        pydot_graph.add_node(node)\n",
    "    for edge in pydot_edges:\n",
    "        pydot_graph.add_edge(\n",
    "            pydot.Edge(pydot_nodes[edge[0]], pydot_nodes[edge[1]]))\n",
    "    return pydot_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def view_graph(prototxt):\n",
    "    _net = caffe_pb2.NetParameter()\n",
    "    f = open(prototxt)\n",
    "    text_format.Merge(f.read(), _net)\n",
    "    display(Image(get_pydot_graph(_net,\"TB\").create_png()))\n",
    "    \n",
    "view_graph(\"GoingDeep/hdf5_classification/logreg_auto_test.prototxt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "* Do you notice any differences respect the previous graph?\n",
    "* What's the meaning of the \"2\" above ip1 node?\n",
    "* Where are the `blobs`?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the solver\n",
    "Now, we'll define our \"solver\" which trains the network by specifying the locations of the train and test nets we defined above, as well as setting values for various parameters used for learning, display, and \"snapshotting\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe.proto import caffe_pb2\n",
    "\n",
    "class create_solver(object):\n",
    "    def __init__(self, train_net_path, test_net_path):\n",
    "        self.train_net_path = train_net_path\n",
    "        self.test_net_path = test_net_path\n",
    "        \n",
    "        self.define_solver()\n",
    "    \n",
    "    def define_solver(self):\n",
    "        self.s = caffe_pb2.SolverParameter()\n",
    "\n",
    "        # Specify locations of the train and test networks.\n",
    "        self.s.train_net = self.train_net_path\n",
    "        self.s.test_net.append(self.test_net_path)\n",
    "\n",
    "        self.s.test_interval = 100  # Test after every 100 training iterations.\n",
    "        self.s.test_iter.append(250) # Test 250 \"batches\" each time we test.\n",
    "\n",
    "        self.s.max_iter = 10000      # # of times to update the net (training iterations)\n",
    "\n",
    "        # Set the initial learning rate for stochastic gradient descent (SGD).\n",
    "        self.s.base_lr = 0.1        \n",
    "        # Set `lr_policy` to define how the learning rate changes during training.\n",
    "        # Here, we 'step' the learning rate by multiplying it by a factor `gamma`\n",
    "        # every `stepsize` iterations.\n",
    "        self.s.lr_policy = 'step'\n",
    "        self.s.gamma = 0.1\n",
    "        self.s.stepsize = 5000\n",
    "\n",
    "        # Set other optimization parameters. Setting a non-zero `momentum` takes a\n",
    "        # weighted average of the current gradient and previous gradients to make\n",
    "        # learning more stable. L2 weight decay regularizes learning, to help prevent\n",
    "        # the model from overfitting.\n",
    "        self.s.momentum = 0.9\n",
    "        self.s.weight_decay = 5e-4\n",
    "\n",
    "        # Display the current training loss and accuracy every 1000 iterations.\n",
    "        self.s.display = 100\n",
    "\n",
    "        # Snapshots are files used to store networks we've trained.  Here, we'll\n",
    "        # snapshot every 10K iterations -- just once at the end of training.\n",
    "        # For larger networks that take longer to train, you may want to set\n",
    "        # snapshot < max_iter to save the network and training state to disk during\n",
    "        # optimization, preventing disaster in case of machine crashes, etc.\n",
    "        self.s.snapshot = 10000\n",
    "        self.s.snapshot_prefix = 'GoingDeep/hdf5_classification/data/train'\n",
    "\n",
    "        # We'll train on the CPU for fair benchmarking against scikit-learn.\n",
    "        # Changing to GPU should result in much faster training!\n",
    "        self.s.solver_mode = caffe_pb2.SolverParameter.CPU\n",
    "\n",
    "        return self.s\n",
    "\n",
    "    def write_solver(self, solver_path):\n",
    "        with open(solver_path, 'w') as f:\n",
    "            f.write(str(self.s))\n",
    "        \n",
    "solver_path = 'GoingDeep/hdf5_classification/logreg_solver.prototxt'\n",
    "s = create_solver(train_net_path, test_net_path)       \n",
    "s.write_solver(solver_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to learn and evaluate our Caffeinated logistic regression in Python.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "caffe.set_mode_cpu()\n",
    "solver = caffe.get_solver(solver_path)\n",
    "solver.solve()\n",
    "\n",
    "accuracy = 0\n",
    "batch_size = solver.test_nets[0].blobs['data'].num\n",
    "test_iters = int(len(Xt) / batch_size)\n",
    "for i in range(test_iters):\n",
    "    solver.test_nets[0].forward()\n",
    "    accuracy += solver.test_nets[0].blobs['accuracy'].data\n",
    "accuracy /= test_iters\n",
    "\n",
    "print(\"Accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization \n",
    "It's very simple to access to the weight, parameters, and intermediate results that are attached to a network graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# weights connecting the input with ip1\n",
    "arr = solver.net.params[\"ip1\"][0].data\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(arr, interpolation='none')\n",
    "fig.colorbar(cax, orientation=\"horizontal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(arr.tolist(), bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best parameters\n",
    "Now try to hack the code above in order to find the best parameters (remember the previous lab, and how perform a grid search). I'm shure you could reach at least an accuracy of: `0.876`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hint: edit the parameters by directly access the create_solver class data structure \n",
    "s = create_solver(train_net_path, test_net_path)       \n",
    "s.s.base_lr = 0.1                      # change the learning rate\n",
    "s.write_solver(solver_path)            # recreate the solver prototxt\n",
    "\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the impact of `learning rate` and `weight decay` regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could always do the same through the command line interface, for detailed output on the model and solving. Remember to correctly change the path according to your directories tree if you are not using the provided docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!/opt/caffe/build/tools/caffe train -solver GoingDeep/hdf5_classification/logreg_solver.prototxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Non linearity\n",
    "\n",
    "If you look at output or the `logreg_auto_train.prototxt`, you'll see that the model is simple logistic regression.\n",
    "We can make it a little more advanced by introducing a non-linearity between weights that take the input and weights that give the output -- now we have a two-layer network.\n",
    "That network is given in `nonlinear_auto_train.prototxt`, and that's the only change made in `nonlinear_logreg_solver.prototxt` which we will now use.\n",
    "\n",
    "The final accuracy of the new network should be higher than logistic regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "\n",
    "def nonlinear_net(hdf5, batch_size, dim):\n",
    "    # one small nonlinearity, one leap for model kind\n",
    "    n = caffe.NetSpec()\n",
    "    n.data, n.label = L.HDF5Data(batch_size=batch_size, source=hdf5, ntop=2)\n",
    "    # define a hidden layer of dimension 40\n",
    "    n.ip1 = L.InnerProduct(n.data, num_output=dim, weight_filler=dict(type='xavier'))\n",
    "    # transform the output through the ReLU (rectified linear) non-linearity\n",
    "    n.relu1 = L.ReLU(n.ip1, in_place=True)\n",
    "    # score the (now non-linear) features\n",
    "    n.ip2 = L.InnerProduct(n.ip1, num_output=2, weight_filler=dict(type='xavier'))\n",
    "    # same accuracy and loss as before\n",
    "    n.accuracy = L.Accuracy(n.ip2, n.label)\n",
    "    n.loss = L.SoftmaxWithLoss(n.ip2, n.label)\n",
    "    return n.to_proto()\n",
    "\n",
    "def predict_net(hdf5, batch_size,dim):\n",
    "    # one small nonlinearity, one leap for model kind\n",
    "    n = caffe.NetSpec()\n",
    "    n.data = L.HDF5Data(batch_size=batch_size, source=hdf5, ntop=1)\n",
    "    # define a hidden layer of dimension 40\n",
    "    n.ip1 = L.InnerProduct(n.data, num_output=dim, weight_filler=dict(type='xavier'))\n",
    "    # transform the output through the ReLU (rectified linear) non-linearity\n",
    "    n.relu1 = L.ReLU(n.ip1, in_place=True)\n",
    "    # score the (now non-linear) features\n",
    "    n.ip2 = L.InnerProduct(n.ip1, num_output=2, weight_filler=dict(type='xavier'))\n",
    "    # same accuracy and loss as before\n",
    "    return n.to_proto()\n",
    "\n",
    "train_net_path = 'GoingDeep/hdf5_classification/nonlinear_auto_train.prototxt'\n",
    "with open(train_net_path, 'w') as f:\n",
    "    f.write(str(nonlinear_net('GoingDeep/hdf5_classification/data/train.txt', 10, 40)))\n",
    "\n",
    "test_net_path = 'GoingDeep/hdf5_classification/nonlinear_auto_test.prototxt'\n",
    "with open(test_net_path, 'w') as f:\n",
    "    f.write(str(nonlinear_net('GoingDeep/hdf5_classification/data/test.txt', 10, 40)))\n",
    "    \n",
    "submission_net_path = 'GoingDeep/hdf5_classification/nonlinear_auto_submission.prototxt'\n",
    "with open(submission_net_path, 'w') as f:\n",
    "    f.write(str(predict_net('GoingDeep/hdf5_classification/data/submission.txt', 10, 40)))\n",
    "\n",
    "solver_path = 'GoingDeep/hdf5_classification/nonlinear_logreg_solver.prototxt'\n",
    "s = create_solver(train_net_path, test_net_path)\n",
    "s.s.base_lr = 0.01\n",
    "s.write_solver(solver_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the network graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "view_graph(\"GoingDeep/hdf5_classification/nonlinear_auto_test.prototxt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# It's your turn\n",
    "# Try to improve your score!\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Find the best parameters!\n",
    "* Can you breake the wall of `0.9` of accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Let's go deeper\n",
    "\n",
    "Try to pack many layers in order to increase your model capacity. But pay attention that, the more you increase your model's capacity, the more you could overfit your training data!\n",
    "Look carefully at the image below:\n",
    "![Capacity](capacity.png \"Capacity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Have you got the point? Now try to create a model with an optimal capacity according to you data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "\n",
    "drop = 0.5\n",
    "def nonlinear_net(hdf5, batch_size):\n",
    "    # one small nonlinearity, one leap for model kind\n",
    "    n = caffe.NetSpec()\n",
    "    n.data, n.label = L.HDF5Data(batch_size=batch_size, source=hdf5, ntop=2)\n",
    "    # define a hidden layer of dimension 40\n",
    "    n.ip1 = L.InnerProduct(n.data, num_output=20, weight_filler=dict(type='xavier'))\n",
    "    # transform the output through the ReLU (rectified linear) non-linearity\n",
    "    n.relu1 = L.ReLU(n.ip1, in_place=True)\n",
    "    # define a hidden layer of dimension 20\n",
    "    n.ip2 = L.InnerProduct(n.ip1, num_output=10, weight_filler=dict(type='xavier'))\n",
    "    # transform the output through the ReLU (rectified linear) non-linearity\n",
    "    n.relu2 = L.ReLU(n.ip2, in_place=True)\n",
    "    n.drop2 = L.Dropout(n.relu2, dropout_ratio = drop, in_place=True)\n",
    "     \n",
    "    # score the (now non-linear) features\n",
    "    n.ip3 = L.InnerProduct(n.drop2, num_output=2, weight_filler=dict(type='xavier'))\n",
    "    # same accuracy and loss as before\n",
    "    n.accuracy = L.Accuracy(n.ip3, n.label)\n",
    "    n.loss = L.SoftmaxWithLoss(n.ip3, n.label)\n",
    "    return n.to_proto()\n",
    "\n",
    "def predict_net(hdf5, batch_size):\n",
    "    # one small nonlinearity, one leap for model kind\n",
    "    n = caffe.NetSpec()\n",
    "    n.data, n.label = L.HDF5Data(batch_size=batch_size, source=hdf5, ntop=2)\n",
    "    # define a hidden layer of dimension 40\n",
    "    n.ip1 = L.InnerProduct(n.data, num_output=20, weight_filler=dict(type='xavier'))\n",
    "    # transform the output through the ReLU (rectified linear) non-linearity\n",
    "    n.relu1 = L.ReLU(n.ip1, in_place=True)\n",
    "    # define a hidden layer of dimension 20\n",
    "    n.ip2 = L.InnerProduct(n.ip1, num_output=10, weight_filler=dict(type='xavier'))\n",
    "    # transform the output through the ReLU (rectified linear) non-linearity\n",
    "    n.relu2 = L.ReLU(n.ip2, in_place=True)\n",
    "    n.drop2 = L.Dropout(n.relu2, dropout_ratio = drop, in_place=True)\n",
    "     \n",
    "    # score the (now non-linear) features\n",
    "    n.ip3 = L.InnerProduct(n.drop2, num_output=2, weight_filler=dict(type='xavier'))\n",
    "    # same accuracy and loss as before\n",
    "    return n.to_proto()\n",
    "\n",
    "train_net_path = 'GoingDeep/hdf5_classification/nonlinear_auto_train.prototxt'\n",
    "with open(train_net_path, 'w') as f:\n",
    "    f.write(str(nonlinear_net('GoingDeep/hdf5_classification/data/train.txt', 10)))\n",
    "\n",
    "test_net_path = 'GoingDeep/hdf5_classification/nonlinear_auto_test.prototxt'\n",
    "with open(test_net_path, 'w') as f:\n",
    "    f.write(str(nonlinear_net('GoingDeep/hdf5_classification/data/test.txt', 10)))\n",
    "    \n",
    "submission_net_path = 'GoingDeep/hdf5_classification/nonlinear_auto_submission.prototxt'\n",
    "with open(submission_net_path, 'w') as f:\n",
    "    f.write(str(predict_net('GoingDeep/hdf5_classification/data/submission.txt', 10)))\n",
    "\n",
    "solver_path = 'GoingDeep/hdf5_classification/nonlinear_logreg_solver.prototxt'\n",
    "s = create_solver(train_net_path, test_net_path)\n",
    "s.s.base_lr = 0.01\n",
    "s.s.gamma = 0.5\n",
    "s.s.stepsize = 10000\n",
    "s.s.max_iter = 50000\n",
    "s.write_solver(solver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "view_graph(\"GoingDeep/hdf5_classification/nonlinear_auto_test.prototxt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "caffe.set_mode_cpu()\n",
    "solver = caffe.get_solver(solver_path)\n",
    "solver.solve()\n",
    "\n",
    "\n",
    "accuracy = 0\n",
    "batch_size = solver.test_nets[0].blobs['data'].num\n",
    "test_iters = int(len(Xt) / batch_size)\n",
    "for i in range(test_iters):\n",
    "    solver.test_nets[0].forward()\n",
    "    accuracy += solver.test_nets[0].blobs['accuracy'].data\n",
    "accuracy /= test_iters\n",
    "\n",
    "print(\"Accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting iteration accuracy\n",
    "\n",
    "There are many way to produce data to be used in order to plot the iteration accuracy. One of these is to export the output of the learning phase in a file, and then parse it with `parse_log.py` script, that it is already provided in caffe.\n",
    "\n",
    "**!Remember to edit the path if you do not use docker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!/opt/caffe/build/tools/caffe train -solver GoingDeep/hdf5_classification/nonlinear_logreg_solver.prototxt 2>&1 | tee lenet_train.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python /opt/caffe/tools/extra/parse_log.py lenet_train.log ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_log = pd.read_csv(\"./lenet_train.log.train\")\n",
    "test_log = pd.read_csv(\"./lenet_train.log.test\")\n",
    "_, ax1 = plt.subplots(figsize=(15, 8))\n",
    "ax2 = ax1.twinx()\n",
    "ax3 = ax1.twinx()\n",
    "\n",
    "ax1.plot(train_log[\"NumIters\"], train_log[\"LearningRate\"], alpha=0.4)\n",
    "ax1.plot(test_log[\"NumIters\"], test_log[\"loss\"], 'g')\n",
    "ax2.plot(test_log[\"NumIters\"], test_log[\"accuracy\"], 'r')\n",
    "ax3.plot(train_log[\"NumIters\"], train_log[\"accuracy\"], 'b')\n",
    "\n",
    "# try to add also the LearningRate!\n",
    "\n",
    "ax1.axis([0, 50000, 0, 1])\n",
    "ax2.axis([0, 50000, 0, 1])\n",
    "ax3.axis([0, 50000, 0, 1])\n",
    "\n",
    "ax1.set_xlabel('iteration')\n",
    "ax1.set_ylabel('train loss')\n",
    "ax2.set_ylabel('test accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready for the submission?\n",
    "\n",
    "Let's try a prediction! In order to do it, you just need to initialize `caffe.Net` with the prototxt and the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = caffe.Net ('GoingDeep/hdf5_classification/nonlinear_auto_submission.prototxt',\n",
    "                 'GoingDeep/hdf5_classification/data/train_iter_50000.caffemodel',\n",
    "                caffe.TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you call the `forward()` method for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i in xrange(900):\n",
    "    pred = net.forward()\n",
    "    labels.append(net.blobs['ip3'].data[...].argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How the output looks like?\n",
    "* What does the `.argmax()`?\n",
    "* Try to reshape the `labels` in order to have a vector 1x9000 (Recall numpy Lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = # It's your turn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('submission.csv', 'wb') as csvfile:\n",
    "    for i in xrange(9000):\n",
    "        csvfile.write(\"{0},\".format(int(y_pred[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clean up (comment this out if you want to examine the hdf5_classification/data directory).\n",
    "shutil.rmtree(dirname)"
   ]
  }
 ],
 "metadata": {
  "description": "Use Caffe as a generic SGD optimizer to train logistic regression on non-image HDF5 data.",
  "example_name": "Off-the-shelf SGD for classification",
  "include_in_docs": true,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "priority": 4,
  "widgets": {
   "state": {},
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
